---
title: "POMDP Planning and Learning in Uncertain Systems"
author: Carl Boettiger and Milad Memarzadeh
output: 
  github_document:
    html_preview: false
---

[![Build Status](https://drone.carlboettiger.info/api/badges/boettiger-lab/pomdpplus/status.svg)](https://drone.carlboettiger.info/boettiger-lab/pomdpplus)

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```


```{r}
library("pomdpplus")
library("ggplot2")
library("tidyr")
library("dplyr")
library("seewave")
```

Initialize a simple POMDP model for fisheries:

```{r}
states <- 0:10
reward_fn <-  function(x, a) pmin(x, a)
sigma_g = 0.28
sigma_m = 0.28

## Create a list of models with these parameters
p <- expand.grid(K = c(4,8), r = c(1/2, 1))

models <- apply(p, 1, function(row) 
  function(x, h){
     s <- pmax(x - h, 0)
     s * exp(row[["r"]] * (1 - s / row[["K"]]) )
  })
    

matrices <- lapply(models, function(model) 
  appl::fisheries_matrices(states = states, actions = states, 
                           observed_states = states, reward_fn = reward_fn,
                           f = model, sigma_g = sigma_g, sigma_m = sigma_m))

T <- lapply(matrices, `[[`, "transition")
O <- lapply(matrices, `[[`, "observation")
reward <- matrices[[1]][["reward"]] # same for all models
```




```{r}
## In addition to the matrices, to define the POMDP problem we need a discount factor for future rewards
discount = 0.95

## PLUS parameters
t = 100       # We'll simulate 100 steps into the future using these modesl
Num_sim = 20  # We will use 20 replicate simulations
n_true = 3    # We specify model 3 as the "true model"
n_sample = 2  # As the number of models could be large, we will sub-sample for efficiency


## We use uniform priors for the initial belief over states and over models
initial = array(1, dim = length(states)) / length(states)
P = (array(1,dim = length(models))/ length(models))
```


Run PLUS:


```{r}
plus_results <- plus(list(T, O, reward, discount), t, Num_sim, n_true, n_sample, initial, P, precision = 1)
```



```{r}
## extract results to separate objects for convenience
df <- plus_results[["df"]]
posterior <- plus_results[["posterior"]]
```

Have we converged to the correct model?

```{r}
tail(posterior)
```


The data.frame `df` returns information about the simulations for both the true and learned `plus` models, allowing us to compare them. Here we compare the rewards garnered from the true and `plus` model over time, summarizing across the replicate simulations. 

```{r}
df %>% 
  select(time, sim, true_reward, plus_reward) %>%
  gather(model, value, -time, -sim) %>%
  ggplot(aes(time, value, group = model)) + 
    stat_summary(aes(color = model), geom="line", fun.y = mean, lwd = 1) +
    stat_summary(geom="ribbon", fun.data = mean_se, alpha = 0.1)
```


We can also compute the KL diveregnce to show the rate of learning:


```{r}
# delta function for true model distribution
h_star = array(0,dim = length(models)) 
h_star[n_true] = 1

## Fn for the base-2 KL divergence from true model, in a friendly format
kl2 <- function(value) seewave::kl.dist(value, h_star, base = 2)[[2]]

## Compute KL over models
P <- posterior %>% 
  gather(model, value, -time, -sim) %>%
  group_by(time, sim) %>% 
  summarise(kl = kl2(value))

## plot mean/se of KL over replicates
ggplot(P, aes(time, kl)) + 
    stat_summary(geom="line", fun.y = mean, lwd = 1) +
    stat_summary(geom="ribbon", fun.data = mean_se, alpha = 0.1)

```

