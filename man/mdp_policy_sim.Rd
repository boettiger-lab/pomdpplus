% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp_learning.R
\name{mdp_policy_sim}
\alias{mdp_policy_sim}
\title{mdp_policy_sim}
\usage{
mdp_policy_sim(policy, transition, reward, discount, x0, Tmax)
}
\arguments{
\item{policy}{the policy to apply in the simulation}

\item{transition}{a single transition matrix that wil lbe used for the simulation}

\item{reward}{the utility matrix U(x,a) of being at state x and taking action a}

\item{discount}{the discount factor (1 is no discounting)}

\item{x0}{initial state}

\item{Tmax}{length of time to simulate}
}
\value{
a data.frame with the state, action, and value at each time t
}
\description{
Simulate a (static) policy given the transition model
}
\examples{
## Setup
source(system.file("examples/K_models.R", package="pomdpplus"))
transition <- lapply(models, `[[`, "transition")
reward <- models[[1]][["reward"]]

## Simulate how policy over model uncertainty performs when model 1 is true:
df <- compute_mdp_policy(transition, reward, discount)
out <- mdp_policy_sim(df$policy, transition[[1]], reward, discount, x0 = 10, Tmax = 20)
}

