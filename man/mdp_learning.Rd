% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp_learning.R
\name{mdp_learning}
\alias{mdp_learning}
\title{mdp learning}
\usage{
mdp_learning(transition, reward, discount, model_prior = rep(1,
  length(transition))/length(transition), x0, Tmax = 20, true_transition)
}
\arguments{
\item{transition}{list of transition matrices, one per model}

\item{reward}{the utility matrix U(x,a) of being at state x and taking action a}

\item{discount}{the discount factor (1 is no discounting)}

\item{model_prior}{the prior belief over models, a numeric of length(transitions). Uniform by default}

\item{x0}{initial state}

\item{Tmax}{length of time to simulate}

\item{true_transition}{actual transition used to drive simulation}
}
\value{
a list, containing: data frame "df" with the state, action and a value at each time step in the simulation,
and an array "posterior", in which the t'th row shows the belief state at time t.
}
\description{
mdp learning
}
\examples{
source(system.file("examples/K_models.R", package="pomdpplus"))
transition <- lapply(models, `[[`, "transition")
reward <- models[[1]][["reward"]]

## example where true model is model 1
out <- mdp_learning(transition, reward, discount, x0 = 10,
                    Tmax = 20, true_transition = transition[[1]])
## Did we learn which one was the true model?
out$posterior[20,]
}

