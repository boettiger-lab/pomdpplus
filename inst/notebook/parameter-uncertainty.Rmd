---
title: "parameter-uncertainty"
author: "Carl Boettiger"
date: "7/25/2016"
output: 
  html_document:
    keep_md: true
    markdown_flavor: github
---

## Outline


- Uncertainty Planning

  - Illustrate how to integrate over model uncertainty, compare to ignoring this
  - Note that comparing fixing the uncertain parameter to prior mean (i.e. ignoring uncertainty) against the case of the proper integration over model uncertainty provides a more objective comparison of the role of accounting for uncertainty than simply comparing the uncertain case to fixing the uncertain parameter arbitrarily. 

  - Compute policy difference and value-of-perfect-information (VOPI) under various scenarios (e.g. select differences in parameters/models that result in very different optimal escapements).  (e.g. presumably VOPI in noise parameters may not be so much?  Is knowing measurement noise or growth noise more valuable?)
  - Does including a model with a tipping point result in more or less cautious behavior?

- Uncertainty Planning & Learning
  
  - Compare passive learning to no learning when VOPI is large.  
  - How many models can be considered, and how to select which to include? (e.g. those with larger VOPI differences)
  - Compare passive learning to post-hoc inference? Theoretically identical? Does/can passive learning out-perform direct post-hoc model estimation?

- POMDP cases: Consequences of assuming perfect observability
  - Compare applying MDP with model uncertainty when reality is POMDP with model uncertainty. Compare policies predicted by each. 
  - Compare applying MDP (passive) learning with POMDP (passive) learning.  Compare rate of learning & outcomes. 


## Current manuscript:

So far, we implement POMDP Planning (parameter & model uncertainty) & POMDP passive learning.  We compare how: (a) assuming a fixed parameter, (b) integrating over uncertainty, and (c) passive learning of the parameters each compare to the case of knowing the true model/parameters (with growth and measurement error still present.)

We do not compare to MDP cases.  

To do: discuss difference between irreducible noise vs uncertainty about models. 

## Examples




```{r, message=FALSE}
library("pomdpplus")
library("ggplot2")
library("tidyr")
library("dplyr")
```

Initialize a simple POMDP model for fisheries:

```{r}
set.seed(1234)
states <- 0:50
actions <- states
observed_states <- states
reward_fn <-  function(x, a) pmin(x, a)
sigma_g <- 0.28
sigma_m <- 0.28
discount <- 0.95

## Create a list of functions, each function providing the model with different parameter combinations
pars <- expand.grid(K = c(45), r = seq(0, 2, length.out = 11))
models <- apply(pars, 1, function(row) 
  function(x, h){
     s <- pmax(x - h, 0)
     s * exp(row[["r"]] * (1 - s / row[["K"]]) )
  })
    

matrices <- lapply(models, function(model) 
  appl::fisheries_matrices(states = states, actions = actions, 
                           observed_states = states, reward_fn = reward_fn,
                           f = model, sigma_g = sigma_g, sigma_m = sigma_m))


transition <- lapply(matrices, `[[`, "transition")
utility <- matrices[[1]]$reward
Tmax <- 20

## initial prior over models
qt <- rep(1, length(models)) / length(models)
#qt <- numeric(length(models))
#qt[5] <- 1
```


transition is an array of probabilities $P(x_{t+1} | x_t, a_t)$ where `transition[i,j,k]` 
refers to probability for  `x_t = state[i]`, `x_{t+1} = state[j]`, and `a_t = action[k]`.


## Planning for Uncertainty

First we consider the optimal solution where we simply average over the uncertainty


```{r}

value_iteration <- function(transition, utility, qt = rep(1, length(transition))/length(transition), Tmax){
  
  n_models <- length(transition)
  n_states <- dim(transition[[1]])[1]
  n_actions <- dim(transition[[1]])[3]
  
  ## Initialize value-to-go (integrated over models)
  Vtplus <- numeric(n_states)
  ## Initialize policy & final value
  policy <- numeric(n_states)
  ## Intialize value-to-go over states i for model j
  V_model <- array(dim=c(n_states, n_models))
  
  for (t in (Tmax - 1):1) {
  # We define a matrix Q that stores the updated action values for 
  # all states (rows), actions (columns)
    Q <- array(0, dim = c(n_states, n_actions))
    for (i in 1:n_actions) {
    # For each action we fill for all states values (rows) 
    # the value associated with that action (ith column) into matrix Q
    # The utility of the ith action recorded for each state is 
    # added to the discounted average (product) of the transition matrix of the ith 
    # action by the running action value Vt 
      
      ## In the case of parametric/model uncertainty, we compute this value for each model..
      for(j in 1:n_models){
         V_model[,j] <- transition[[j]][,,i] %*% Vtplus
      }
      
      ## and then average over models weighted by qt
      Q[,i] <- utility[, i] + discount * V_model %*% qt
     
    } # end of the actions loop
  
    # Find the optimal action value at time t is the maximum of Q
    Vt <- apply(Q, 1, max)
  
    # After filling vector Vt of the action values at all states, we 
    # update the vector Vt+1 to Vt and we go to the next step standing 
    # for previous time t-1, since we iterate backward
    Vtplus <- Vt
  
  } # end of the time loop
  
  # Find optimal action for each state
  for (k in 1:n_states) {
  # We look for each state which column of Q corresponds to the 
  # maximum of the most recent value, Vt (at time t + 1). 
  # If the index vector is longer than 1, (i.e. if there is more
  # than one equally optimal value) we chose the smaller action (min harvest)
      policy[k] <- min(which(Q[k, ] == Vt[k]))
  }

  list(policy = policy, value = Vt)  
}  
```


```{r}

out <- value_iteration(transition, utility, qt, Tmax)
plot(states, states - actions[out$policy], xlab="Population size", ylab="Escapement")

```



Compare policies arising from different (prior) beliefs (and different models). What is the general impact of parameter uncertainty on the policy?  In the Ricker model, a larger $r$ means a lower optimal escapement. Compare a uniform prior over $r \in [0,2]$ to certainty that r is either very small (0.4) or very large (1.8): 

```{r}
qt_unif <- rep(1, length=length(transition)) / length(transition)
unif <- value_iteration(transition, utility, qt_unif, Tmax)

qt_high <- numeric(length(transition))
qt_high[11] <- 1
certain_high <- value_iteration(transition, utility, qt_high, Tmax)

qt_mean <- numeric(length(transition))
qt_mean[6] <- 1
certain_mean <- value_iteration(transition, utility, qt_mean, Tmax)


qt_low <- numeric(length(transition))
qt_low[2] <- 1
certain_low <- value_iteration(transition, utility, qt_low, Tmax)

```

Not surprisingly, we see that the optimal policy is intermediate (though more pessimistic than had we assumed prior mean):

```{r}
data.frame(states = states, 
           unif = unif$policy, 
           certain_low = certain_low$policy, 
           certain_high = certain_high$policy,
           certain_mean = certain_mean$policy) %>% 
  gather(prior, policy, -states) %>%
  ggplot(aes(states, states - actions[policy], col=prior, lty=prior)) + 
  geom_line() + ylab("escapement")


```




### Value of Perfect Information

From this, we can also define the value of perfect information.  
```{r}

value_of_policy <- function(policy, transition, utility, qt = rep(1, length(transition))/length(transition), Tmax){
  n_models <- length(transition)
  n_states <- dim(transition[[1]])[1]
  n_actions <- dim(transition[[1]])[3]
  Vt <- numeric(n_states)
  V_model <- array(dim=c(n_states, n_models))
  
  for (t in (Tmax - 1):1) {
    Q <- array(0, dim = c(n_states, n_actions))
    for (i in 1:n_actions) {
      for(j in 1:n_models){
         V_model[,j] <- transition[[j]][,,i] %*% Vt
      }
      Q[,i] <- utility[, i] + discount * V_model %*% qt
    }
    
    for(i in 1:n_states)
      Vt[i] <- Q[i,policy[i]]
    #Vt <- apply(Q, 1, max)
  } 
  Vt
}
```


```{r}

  low_low = value_of_policy(certain_low$policy, transition, utility, qt_low, Tmax = 200)
  high_high = value_of_policy(certain_high$policy, transition, utility, qt_high, Tmax = 200)

  unif_low = value_of_policy(unif$policy, transition, utility, qt_low, Tmax = 200) / low_low
  high_low = value_of_policy(certain_high$policy, transition, utility, qt_low, Tmax = 200)  / low_low
  
  unif_high = value_of_policy(unif$policy, transition, utility, qt_high, Tmax = 200) / high_high
  low_high = value_of_policy(certain_low$policy, transition, utility, qt_high, Tmax = 200)  / high_high
  
  data.frame(state = states, unif_low = unif_low, unif_high = unif_high, low_high = low_high, high_low = high_low) %>%
    gather(model, value, -state) %>%
    ggplot(aes(state, value, col = model)) + geom_point()
```
  
  

```{r}
  n_states <- length(states)
  unif_low[n_states]
  unif_high[n_states]
```

The expected or average reward from the policy assuming only a uniform prior of r when the true r is high is `r 100 * unif_high[n_states]`% of perfect information, and when true r is low, this policy similarly gives `r 100 * unif_low[n_states]`% of perfect information. Consequently, additional learning offers the potential of only marginally improved rewards in this scenario over planning for uncertainty without learning.  

However, accounting for model uncertainty can make a substantial difference in this case: using the low estimate of r would give a value of only `r 100 * low_high[n_states]`


```{r}
sim_policy <- function(transition, utility, policy, x0, Tmax){
  state <- action <- value <- numeric(Tmax)
  time <- 1:(Tmax-1)
  state[1] <- x0
  for(t in time){
    
    action[t] <- policy[state[t]] 
    value[t] <- utility[state[t], action[t]] * discount^(t-1)
    prob <- transition[state[t], , action[t]]
    state[t+1] <- sample(1:n_states, 1, prob = prob)
    
    
  }
  
  data.frame(time, state, action, value)
}
```

```{r}
data.frame(states = states, 
           unif = unif$value, 
           certain_low = certain_low$value, 
           certain_high = certain_high$value,
           certain_mean = certain_mean$value) %>% 
  gather(prior, value, -states) %>%
  ggplot(aes(states, value, col=prior, lty=prior)) + 
  geom_line() + ylab("value")


```



...

Thus far these are stationary policies -- the policy is a function of the observed state, but given the observation the policy is fixed -- no learning takes place from these observations.  

Now we consider updating our belief over the models `qt` in light of each observation.  We begin with passive learning: we will update `qt` in response to each observation, but not take actions to actively 'explore' and accelerate learning.  


```{r}

grid_i <-function(x, grid){
  which.min( abs(x - grid)) 
}

bayes_update_qt <- function(qt, x_t, x_t1, a_t, transition){

  n_models <- length(transition)
  P <- vapply(1:n_models, function(m) transition[[m]][x_t,x_t1,a_t], numeric(1))
  qt * P / sum(qt * P)
  
}
```


```{r}
## without harvest, system goes from state 3->3, (states[4] == 3), small r is more likely
qt1 <- bayes_update_qt(qt, 4, 4, 1, transition)
plot(seq_along(qt1), qt1)



## without harvest, system goes from state 3->6, middle r is likely
qt1 <- bayes_update_qt(qt, 4, 7, 1, transition)
plot(seq_along(qt1), qt1)



## with harvest=2, system goes from state 3->6, large r is most likely
qt1 <- bayes_update_qt(qt, 4, 7, 3, transition)
plot(seq_along(qt1), qt1)

## Compare actions that learn more or less? Determine optimal exploration, value aside?

```




Learn `r` passively over time:


```{r}
## Note that action & state are all given as index values to their respective vectors
mdp_learning <- function(initial_state_index, transition, utility, 
                         qt = rep(1, length(transition)) / length(transition), 
                         Tmax = 20, true_transition){

  n_states <- dim(transition[[1]])[1]
  state <- numeric(Tmax)
  action <- numeric(Tmax)
  value <- numeric(Tmax)
  q <- array(NA, dim = c(Tmax, length(transition)))
  q[1,] <- qt
  state[1] <- initial_state_index
  
  
  for(t in 1:(Tmax-1)){
  
    ## Determine optimal action
    out <- value_iteration(transition, utility, q[t,], Tmax-t)
    ## Take proposed action, collect discounted reward
    action[t] <- out$policy[state[t]] 
    value[t] <- utility[state[t], action[t]] * discount^(t-1)
    ## draw new state based on true model probability
    prob <- true_transition[state[t], , action[t]]
    state[t+1] <- sample(1:n_states, 1, prob = prob)
    
    ## Update belief
    q[t+1,] <- bayes_update_qt(q[t,], state[t], state[t+1], action[t], transition)
    
    
  }

  # Final action and associated value
  # action[t+1] <- out$policy[state[t+1]] 
  # value[t+1] <- utility[state[t+1], action[t+1]]
    
  
  list(df = data.frame(time = 1:Tmax, state = state, action = action, value = value), posterior = q)
}
```



```{r}
true_i <- 3
ex <- mdp_learning(initial_state_index = length(states), 
                   transition, utility, 
                   qt = rep(1, length(transition)) / length(transition), 
                   Tmax = 50, 
                   true_transition = transition[[true_i]])
```


Illustrate change in prior over time as learning takes place:

```{r}
df <- data.frame(r = pars$r, 
                 quarter = ex$posterior[round(Tmax/4),], 
                 mid = ex$posterior[round(Tmax/2),], 
                 threequarter = ex$posterior[round(3*Tmax/4),], 
                 end = ex$posterior[Tmax,])
ggplot(df) + 
    geom_line(aes(r, quarter), alpha=0.25, lwd = 0.25) + 
    geom_line(aes(r, mid), alpha=0.5, lwd = 0.5) + 
    geom_line(aes(r, threequarter), alpha=0.75, lwd = 0.75) + 
    geom_line(aes(r, end), lwd = 1)
```


What does the policy appear to look like? We compare this to the optimal policy of the true model (see `certain_low`, above).  

```{r}
# (note these are really index values)
ex$df %>% 
ggplot(aes(state, state - action)) + 
  geom_point(aes(col=time)) +
  geom_line(data = data.frame(state = 1:length(states), action = certain_low$policy))
```

We can also see how this policy is applied over time for the realized sequence of growth shocks:


```{r}
ggplot(ex$df) +  geom_line(aes(time, state)) + geom_line(aes(time, action), col = "red")
sum(ex$df$value)
```



A second example, with much higher r as the true model:


```{r}
true_i <- 10
ex <- mdp_learning(initial_state_index = length(states), 
                   transition, utility, 
                   qt = rep(1, length(transition)) / length(transition), 
                   Tmax = 50, 
                   true_transition = transition[[true_i]])
```


```{r}
ggplot(ex$df, aes(state, state - action)) + 
  geom_point() +
  geom_line(data = data.frame(state = 1:length(states), action = certain_high$policy))
```


<!--
Can we invert this to determine a model prior for which the observed action was optimal?  No, since this is many-to-one, e.g. always at least one model for which the action is optimal.  
-->

---------

## Tipping points

```{r}
allen_models <- apply(pars, 1, function(row) 
  function(x, h){
     C <- 10
     s <- pmax(x - h, 0)
     s * exp(row[["r"]] * (1 - s / row[["K"]]) * (s - C) / row[["K"]] )
  })



allen_matrices <- lapply(allen_models, function(model) 
  appl::fisheries_matrices(states = states, actions = actions, 
                           observed_states = states, reward_fn = reward_fn,
                           f = model, sigma_g = sigma_g, sigma_m = sigma_m))

allen_transition <- lapply(allen_matrices, `[[`, "transition")

both_transitions <- c(transition, allen_transition)
```


```{r}
ricker <- value_iteration(transition, utility, Tmax = 20)
allen <- value_iteration(allen_transition, utility, Tmax = 20)
both <- value_iteration(both_transitions, utility, Tmax = 20)
```



```{r}
data.frame(states = states, 
           ricker = states - actions[ricker$policy], 
           allen = states - actions[allen$policy], 
           both = states - actions[both$policy]) %>%
  tidyr::gather(model, escapement, -states) %>% 
  ggplot(aes(states, escapement, col=model)) + geom_line()
```


```{r}
true_i <- 16
ex <- mdp_learning(initial_state_index = length(states), 
                   both_transitions, utility,
                   Tmax = 50, 
                   true_transition = both_transitions[[true_i]])
```


```{r}
q = numeric(length(both_transitions))
q[true_i] <- 1
true <- value_iteration(both_transitions, utility, qt = q, Tmax = 50)
```

```{r}
ex$df %>% 
ggplot(aes(state, state - action)) + 
  geom_point(aes(col=time)) +
  geom_line(data = data.frame(state = 1:length(states), action = true$policy))
```


Learns r but has harder time distinguishing between models:

```{r}
df <- data.frame(model = 1:length(both_transitions), 
                 quarter = ex$posterior[round(Tmax/4),], 
                 mid = ex$posterior[round(Tmax/2),], 
                 threequarter = ex$posterior[round(3*Tmax/4),], 
                 end = ex$posterior[Tmax,])
ggplot(df) + 
    geom_line(aes(model, quarter), alpha=0.25, lwd = 0.25) + 
    geom_line(aes(model, mid), alpha=0.5, lwd = 0.5) + 
    geom_line(aes(model, threequarter), alpha=0.75, lwd = 0.75) + 
    geom_line(aes(model, end), lwd = 1)
```

